{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make BIDS configuration files\n",
    "Natalia VÃ©lez, August 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib, json, yaml, sys\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# Helper functions\n",
    "sys.path.append('..')\n",
    "from utils import str_extract, int_extract "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sessions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210812_teaching_01\n",
      "210812_teaching_02\n",
      "210816_teaching_03\n",
      "210816_teaching_04\n",
      "210821_teaching_05\n",
      "210821_teaching_06\n",
      "210822_teaching_07\n",
      "210822_teaching_08\n",
      "210823_teaching_09\n",
      "210823_teaching_10\n",
      "210823_teaching_11\n",
      "210823_teaching_12\n",
      "210824_teaching_13\n",
      "210824_teaching_14\n",
      "210825_teaching_15\n",
      "210825_teaching_16\n",
      "210825_teaching_17\n",
      "210826_teaching_18\n",
      "210826_teaching_19\n",
      "210826_teaching_20\n",
      "210827_teaching_21\n",
      "210827_teaching_22\n",
      "210829_teaching_23\n",
      "210829_teaching_24\n",
      "210829_teaching_25\n",
      "210830_teaching_26\n",
      "210830_teaching_27\n",
      "210831_teaching_28\n",
      "210831_teaching_29\n",
      "210831_teaching_30\n"
     ]
    }
   ],
   "source": [
    "with open('session_labels.txt', 'r') as f:\n",
    "    sessions = f.read().splitlines()\n",
    "    \n",
    "print(*sessions, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'auth.key'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f0c3ac05d8be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auth.key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'auth.key'"
     ]
    }
   ],
   "source": [
    "with open('auth.key', 'r') as f:\n",
    "    (user, pw) = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data from XNAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session(s):\n",
    "    url_template = 'https://cbscentral.rc.fas.harvard.edu/data/projects/Gershman/subjects/%s/experiments/%s/scans?format=json'\n",
    "    url = url_template % (s,s)\n",
    "    p = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    p.add_password(None, url, user, pw)\n",
    "\n",
    "    handler = urllib.request.HTTPBasicAuthHandler(p)\n",
    "    opener = urllib.request.build_opener(handler)\n",
    "    urllib.request.install_opener(opener)\n",
    "\n",
    "    response = urllib.request.urlopen(url).read()\n",
    "    page = json.loads(response)\n",
    "    session_data = pd.DataFrame(page['ResultSet']['Result'])\n",
    "    \n",
    "    return session_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function: Fix fmap inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intended_for(s, func):\n",
    "    func_runs = s.split(',')\n",
    "    func_runs = [int(r) for r in func_runs]\n",
    "    \n",
    "    func_subset = func[func.scan.isin(func_runs)]    \n",
    "    return func_subset.id.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function: Turn XNAT session info into YAML config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_config(s):\n",
    "    # load inputs\n",
    "    session_data = load_session(s)\n",
    "    session_data = session_data[['ID', 'series_description', 'note']]\n",
    "    session_data['ID'] = session_data.ID.astype(int)\n",
    "\n",
    "    # prepare outputs\n",
    "    out_file = 'outputs/config/%s.yaml' % s\n",
    "    config_data = {'anat': [], 'func': {'bold': []}} # initialize\n",
    "\n",
    "    # anatomical\n",
    "    anat_scans = session_data[session_data.note.str.contains('anat')]\n",
    "    config_data['anat'] = {'T1w': {'scan': int(anat_scans['ID'].values[0]), 'run': 1}}\n",
    "\n",
    "    # functional files\n",
    "    func_scans = session_data[session_data.note.str.contains('teaching|tomloc')]\n",
    "    func_scans['run'] = func_scans.groupby('note').cumcount()+1\n",
    "    func_scans['id'] = func_scans.apply(lambda row: 'task-%s_run-%02d' % (row['note'], row['run']), axis=1)\n",
    "    func_scans = func_scans.rename(columns={'ID': 'scan', 'note': 'task'})\n",
    "    func_scans = func_scans[['scan', 'task', 'run', 'id']]\n",
    "    func_dict = func_scans.to_dict(orient='records')\n",
    "    config_data['func']['bold'] = func_dict\n",
    "\n",
    "    # fieldmaps\n",
    "    fmap_scans = session_data[session_data.series_description.str.contains('FieldMap')]\n",
    "    if fmap_scans.shape[0] > 1:\n",
    "        config_data['fmap'] = {}\n",
    "\n",
    "        fmap_scans = fmap_scans.reset_index(drop=True)\n",
    "        fmap_scans['run'] = fmap_scans.index+1\n",
    "        fmap_scans['direction'] = fmap_scans.series_description.str.extract('(AP|PA)')\n",
    "        fmap_scans['note'] = fmap_scans.note.apply(lambda s: intended_for(s, func_scans))\n",
    "        fmap_scans = fmap_scans.rename(columns={'ID': 'scan', 'note': 'intended_for'})\n",
    "        fmap_scans = fmap_scans[['scan', 'run', 'direction', 'intended_for']]\n",
    "        fmap_scans = fmap_scans.to_dict(orient='records')\n",
    "        config_data['fmap']['epi'] = fmap_scans\n",
    "\n",
    "    # save to file\n",
    "    with open(out_file, 'w') as out:\n",
    "        yaml.dump(config_data, out)\n",
    "        \n",
    "    return config_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through sessions and save outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sessions = {s:session_config(s) for s in sessions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_list = []\n",
    "\n",
    "for sub,ses in all_sessions.items():\n",
    "\n",
    "    # unpacking session information\n",
    "    func_ids = [run['id'] for run in ses['func']['bold']]\n",
    "\n",
    "    # quality checks\n",
    "    n_teaching_runs = len([s for s in ses['func']['bold'] if s['task'] == 'teaching'])\n",
    "    n_tomloc_runs = len([s for s in ses['func']['bold'] if s['task'] == 'tomloc'])\n",
    "    has_anat = 'anat' in ses\n",
    "    has_fmap = 'fmap' in ses\n",
    "    if has_fmap:\n",
    "        n_fmaps = int(len(ses['fmap']['epi'])/2)\n",
    "        fmap_func_ids = np.unique([run for fmap in ses['fmap']['epi'] for run in fmap['intended_for']]).tolist()\n",
    "        fmaps_assigned = func_ids == fmap_func_ids\n",
    "    else:\n",
    "        n_fmaps = 0\n",
    "        fmaps_assigned = False\n",
    "\n",
    "    qa_list.append((sub, n_teaching_runs, n_tomloc_runs, has_anat, has_fmap, n_fmaps, fmaps_assigned))\n",
    "    \n",
    "qa_df = pd.DataFrame(qa_list,\n",
    "                    columns = ('sub', 'n_teaching_runs', 'n_tomloc_runs', 'has_anat',\n",
    "                               'has_fmap', 'n_fmaps', 'fmaps_assigned'))\n",
    "\n",
    "qa_df.to_csv('outputs/protocol_qa.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
