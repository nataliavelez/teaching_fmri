{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teaching task event files\n",
    "Natalia VÃ©lez, March 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join as opj\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils import str_extract, int_extract, gsearch, read_json, write_json\n",
    "\n",
    "sys.path.append('../2_behavioral')\n",
    "import teaching_models as teach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load valid participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_participants = np.loadtxt('../1_preprocessing/outputs/valid_participants.txt')\n",
    "valid_participants = ['sub-%02d' % s for s in valid_participants]\n",
    "print(valid_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla event files (for top-level folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find behavioral files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../'\n",
    "original_files = gsearch(data_dir, 'behavioral', '*.json')\n",
    "original_files = [f for f in original_files if 'run-practice' not in f] # trim out practice runs\n",
    "original_files.sort()\n",
    "\n",
    "print(f'Found {len(original_files)} behavioral files')\n",
    "print(*original_files[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function: Transforms raw data into BIDS-compliant events files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangling(f):\n",
    "\n",
    "    # load input data\n",
    "    in_df = pd.read_json(f)\n",
    "\n",
    "    # copy input data\n",
    "    out_df = (\n",
    "        in_df\n",
    "        .copy()\n",
    "        .rename(columns={\n",
    "            'ons': 'onset',\n",
    "            'dur': 'duration',\n",
    "            'type': 'trial_type',\n",
    "            'problem_idx': 'block_no',\n",
    "            'ex_idx': 'trial_no',\n",
    "            'problem': 'stimuli',\n",
    "            'order': 'stimuli_order',\n",
    "            'rt': 'response_time'\n",
    "        })\n",
    "    )\n",
    "\n",
    "    # create additional rows for motor response\n",
    "    choose_trials = out_df[~pd.isna(out_df.movements)]\n",
    "    rate_trials = out_df[out_df.trial_type == 'rate']\n",
    "\n",
    "    # add cursor movements from choose trials\n",
    "    movement_list = []\n",
    "    for _, row in choose_trials.iterrows():\n",
    "        movements = row.movements\n",
    "        movement_dict = [{'trial_type': 'motor', 'onset': row.onset + m[1], 'duration': 0} for m in movements if m[1] >= 0]\n",
    "        movement_list += movement_dict\n",
    "\n",
    "    # add cursor movements from rate trials\n",
    "    for _, row in rate_trials.iterrows():\n",
    "        movement_dict = {'trial_type': 'motor', 'onset': row.onset+row.response_time, 'duration': 0}\n",
    "        movement_list.append(movement_dict)\n",
    "\n",
    "    # add to main event df\n",
    "    movement_df = pd.DataFrame(movement_list)\n",
    "    out_df = pd.concat([out_df, movement_df])\n",
    "\n",
    "    # make a single \"response\" column\n",
    "    response = np.empty(out_df.shape[0])\n",
    "    response = np.where(out_df.example.isna(), out_df.rating, out_df.example)\n",
    "    out_df['response'] = response\n",
    "\n",
    "    # reorganize df\n",
    "    out_df = (\n",
    "        out_df\n",
    "        .sort_values(by='onset')\n",
    "        .dropna(axis='index', subset=['onset'])\n",
    "        .reset_index(drop=True)\n",
    "        [['onset', 'duration', 'trial_type', 'block_no', 'trial_no', 'stimuli',\n",
    "          'stimuli_order', 'response', 'response_time']]\n",
    "    )\n",
    "    \n",
    "    out_df = out_df[~out_df.trial_type.isin(['pause', 'pre'])] # don't explicitly model fixation periods\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main loop: Iterate through all behavioral files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../BIDS_data/'\n",
    "\n",
    "for f in original_files:\n",
    "    \n",
    "#     print(f'Loading file: {f}')\n",
    "    sub = str_extract('sub-[0-9]{2}', f)\n",
    "    run = str_extract('run-[0-9]{2}', f)\n",
    "    out_f = opj(data_dir, sub, 'func', f'{sub}_task-teaching_{run}_events.tsv')\n",
    "    \n",
    "#     print(f'Saving to: {out_f}\\n')\n",
    "    out_df = data_wrangling(f)\n",
    "    out_df.to_csv(out_f, sep='\\t', index=False, na_rep='n/a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-generated regressors (for GLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg = (\n",
    "    pd.read_csv('../2_behavioral/outputs/model_regressor_values.csv')\n",
    "    .rename(columns={'block_idx': 'block_no', 'ex_idx': 'trial_no', 'problem': 'stimuli'})\n",
    ")\n",
    "model_reg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reg[(model_reg.subject == 1) & (model_reg.run == 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function: Merge model-based regressors with vanilla event files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_reg(f):\n",
    "\n",
    "    # get event data\n",
    "    event_df = data_wrangling(f)\n",
    "    event_df.head()\n",
    "\n",
    "    # get regressors\n",
    "    sub = int_extract('(?<=sub-)[0-9]{2}', f)\n",
    "    run = int_extract('(?<=run-)[0-9]{2}', f)\n",
    "    run_reg = model_reg[(model_reg.subject == sub) & (model_reg.run == run)]\n",
    "    run_reg = run_reg[['block_no', 'trial_no', 'pTrue', 'KL']]\n",
    "\n",
    "    # split \"show\" trials based on whether a hint was selected\n",
    "    choose_trials = event_df[event_df.trial_type == 'choose']\n",
    "    missed_trial = pd.isnull(choose_trials.response)\n",
    "    show_renamed = np.where(missed_trial, 'show_missed', 'show_new')\n",
    "    event_df.loc[event_df.trial_type == 'show', 'trial_type'] = show_renamed\n",
    "\n",
    "\n",
    "    # merge with event files\n",
    "    show_trials = (\n",
    "        event_df\n",
    "        [event_df.trial_type == 'show_new']\n",
    "        .copy()\n",
    "        .drop(columns='trial_type')\n",
    "        .merge(run_reg, on=['block_no', 'trial_no'], how='left')\n",
    "        .melt(id_vars=['onset', 'duration', 'block_no', 'trial_no', 'stimuli', 'stimuli_order', 'response', 'response_time'],\n",
    "              value_vars=['pTrue', 'KL'], var_name='trial_type', value_name='value')\n",
    "    )\n",
    "\n",
    "    # put everything together\n",
    "    model_events = pd.concat([event_df, show_trials])\n",
    "    model_events = (\n",
    "        model_events\n",
    "        .assign(trial_type=model_events.trial_type.astype('category').cat.reorder_categories(event_df.trial_type.unique().tolist() + ['pTrue', 'KL']))\n",
    "        .sort_values(by=['onset', 'duration', 'trial_type'])\n",
    "        [['onset', 'duration', 'stimuli', 'trial_type', 'value']]\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    return model_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../../BIDS_data/derivatives/model_events'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "for f in original_files:\n",
    "    sub = str_extract('sub-[0-9]{2}', f)\n",
    "    run = str_extract('run-[0-9]{2}', f)\n",
    "    out_f = opj(out_dir, sub, 'func', f'{sub}_task-teaching_{run}_model-main_events.tsv')\n",
    "    \n",
    "    if sub in valid_participants:\n",
    "        print(f'Loading file: {f}')\n",
    "        out_df = make_model_reg(f)\n",
    "        \n",
    "        print(f'Saving to: {out_f}\\n')\n",
    "        os.makedirs(opj(out_dir, sub, 'func'), exist_ok=True)\n",
    "        out_df.to_csv(out_f, sep='\\t', index=False, na_rep='n/a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory: Identifiable vs. non-identifiable problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function: Does a problem contain pixels that are unique to the correct answer? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_identifiable(prob):\n",
    "\n",
    "    nonzero_indices = [np.ravel_multi_index(np.nonzero(h), (6,6)) for h in prob.values()]\n",
    "    unique_points = [len(np.setdiff1d(nonzero_indices[0], alt_h)) for alt_h in nonzero_indices[1:]]\n",
    "\n",
    "    return all(unique_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check: Which problems are identifiable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifiable_bool = [is_identifiable(prob) for prob in teach.problems]\n",
    "print(f'{sum(identifiable_bool)} problems are uniquely identifiable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks: Plot an identifiable problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ident_idx = 0\n",
    "print(identifiable_bool[ident_idx])\n",
    "_ = teach.plot_problem(ident_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot a non-identifiable problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonident_idx = 2\n",
    "print(identifiable_bool[nonident_idx])\n",
    "_ = teach.plot_problem(nonident_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = original_files[0]\n",
    "data_wrangling(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-py3]",
   "language": "python",
   "name": "conda-env-.conda-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
